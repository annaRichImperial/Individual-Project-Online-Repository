{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#access data from BNC for more accurate frequency analysis (for british english)\n",
    "#takes a long time to run (a few hours) because of large amount of data and computation\n",
    "\n",
    "#add average frequency per speech?\n",
    "\n",
    "from __future__ import division\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "#from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    bnc_reader = BNCCorpusReader(root=\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\", \n",
    "                            fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "    \n",
    "    list_of_fileids = []\n",
    "    \n",
    "    #calls function to append all files to list of fileids\n",
    "    fill_list_of_fileids(list_of_fileids)        \n",
    "    \n",
    "    #accesses all words from files \n",
    "    words = bnc_reader.words(list_of_fileids)\n",
    "    \n",
    "    #generates the frequency of each word (all lowercase)\n",
    "    #this is the part that takes a long time, due to large number of words\n",
    "    fdist = nltk.FreqDist(w.lower() for w in words)\n",
    "    \n",
    "    \n",
    "    #print(fdist.most_common(50))\n",
    "               \n",
    "    #set up Queen's speech corpus    \n",
    "    newcorpus = PlaintextCorpusReader('data', '.*')\n",
    "    files = newcorpus.fileids()\n",
    "    \n",
    "    # define punctuation\n",
    "    punctuations = '''!()-–[]{};:'\"“”\\,<>’./?@#$%^&*_~'''\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    count_list = []\n",
    "    adj_count_list = []\n",
    "    verb_count_list = []\n",
    "    noun_count_list = []\n",
    "    no_noun_list = []\n",
    "    adverb_count_list = []\n",
    "    \n",
    "    \n",
    "    #for each speech, look at frequency of words used (frequency according to BNC)\n",
    "    for f in files:\n",
    "        \n",
    "        print('---------------------------------------------------')\n",
    "        print(f)\n",
    "        \n",
    "        clean_list = []\n",
    "        word_freq = []\n",
    "        adj_list = []\n",
    "        verb_list = []\n",
    "        noun_list = []\n",
    "        no_noun_list = []\n",
    "        adverb_list = []\n",
    "        \n",
    "        #access and clean data\n",
    "        str1 = clean_data(f, newcorpus)\n",
    "        \n",
    "        #tokenize and tag words\n",
    "        text = nltk.word_tokenize(str1)\n",
    "        pos_words = nltk.pos_tag(text)\n",
    "            \n",
    "        \n",
    "        #for every word in speech, append to relevant list according to word-type tag\n",
    "        for words in pos_words:            \n",
    "            if words[0] not in punctuations: \n",
    "                clean_list.append(words[0])\n",
    "                if (words[1] == 'JJ' or words[1] == 'JJR' or words[1] == 'JJS'):\n",
    "                    adj_list.append(words[0])\n",
    "                if (words[1] == 'VB' or words[1] == 'VBD' or words[1] == 'VBP') \\\n",
    "                or (words[1] == 'VBG' or words[1] == 'VBN' or words[1] == 'VBZ'):\n",
    "                    verb_list.append(words[0])   \n",
    "                if (words[1] == 'RB' or words[1] == 'RBR' or words[1] == 'RBS'):\n",
    "                    adverb_list.append(words[0])\n",
    "                \n",
    "        for words in pos_words:            \n",
    "            if words[0] not in punctuations:\n",
    "                if (words[1] == 'NN' or words[1] == 'NNP' or words[1] == 'NNS'):\n",
    "                    noun_list.append(words[0])    \n",
    "                else:\n",
    "                    no_noun_list.append(words[0])\n",
    "            \n",
    "        abs_freq_list = {}\n",
    "        adj_freq_list = {}\n",
    "        verb_freq_list = {}\n",
    "        noun_freq_list = {}\n",
    "        no_noun_freq_list = {}\n",
    "        adverb_freq_list = {}\n",
    "        \n",
    "        #gets frequency stats for all words in speeches (TODO: compare frequency to BNC frequency)\n",
    "        fdist_queen = nltk.FreqDist(w.lower() for w in newcorpus.words(f))\n",
    "        \n",
    "        #for each word in speech, get BNC frequency and add to dictionary {word :  BNC-frequency}\n",
    "        #abs_freq_list is all words and their frequencies\n",
    "        #additionally separate words into their respective word-types \n",
    "        \n",
    "        for l in fdist_queen:\n",
    "            if l in clean_list:    \n",
    "                \n",
    "                frequency = fdist[l]\n",
    "                abs_freq_list.update({l : frequency}) \n",
    "                word_freq.append(frequency)\n",
    "              \n",
    "                if l in adj_list:\n",
    "                    adj_freq_list.update({l : frequency})\n",
    "                \n",
    "                if l in verb_list:\n",
    "                    verb_freq_list.update({l : frequency})\n",
    "                \n",
    "                if l in noun_list:\n",
    "                    noun_freq_list.update({l : frequency})\n",
    "               \n",
    "                if l in no_noun_list:\n",
    "                    no_noun_freq_list.update({l : frequency})\n",
    "                    \n",
    "                if l in adverb_list:\n",
    "                    adverb_freq_list.update({l : frequency})\n",
    "        \n",
    "        \n",
    "        low_freq_count = 0\n",
    "        adj_freq_count = 0\n",
    "        verb_freq_count = 0\n",
    "        noun_freq_count = 0\n",
    "        no_noun_freq_count = 0\n",
    "        adverb_freq_count = 0\n",
    "        \n",
    "        #goes through each item in respective word-type lists, and counts the number of occurrences\n",
    "        #of said word-types below a certain frequency\n",
    "        \n",
    "        #range can be altered for different frequency \"cut-offs\"\n",
    "        #e.g. here, it will only count those words with frequencies < 200\n",
    "        for num in range(200):\n",
    "            for k, v in abs_freq_list.items():\n",
    "                if v == num:\n",
    "                    low_freq_count += 1\n",
    "                    print(k, v)\n",
    "                    \n",
    "            for k, v in adj_freq_list.items():\n",
    "                if v == num:\n",
    "                    adj_freq_count += 1\n",
    "                    print(k, v)\n",
    "                    \n",
    "            for k, v in verb_freq_list.items():\n",
    "                if v == num:\n",
    "                    verb_freq_count += 1\n",
    "                    print(k, v)\n",
    "                    \n",
    "            for k, v in noun_freq_list.items():\n",
    "                if v == num:\n",
    "                    noun_freq_count += 1\n",
    "                    print(k, v)\n",
    "                    \n",
    "            for k, v in no_noun_freq_list.items():\n",
    "                if v == num:\n",
    "                    no_noun_freq_count += 1\n",
    "                    print(k, v)     \n",
    "                    \n",
    "            for k, v in adverb_freq_list.items():\n",
    "                if v == num:\n",
    "                    adverb_freq_count += 1\n",
    "                    print(k, v)\n",
    "        \n",
    "        \n",
    "        #proportion of low frequency words in each text appended to list\n",
    "        #displayed on graph to see changes over time\n",
    "        \n",
    "        #LOW-COUNT-PROPORTION\n",
    "        count_list.append((low_freq_count/len(text))*100)\n",
    "        \n",
    "       \n",
    "        #ADJ-LOW-COUNT PROPORTION\n",
    "        adj_count_list.append((adj_freq_count/len(text))*100)\n",
    "        \n",
    "       \n",
    "        #VERB-LOW-COUNT PROPORTION\n",
    "        verb_count_list.append((verb_freq_count/len(text))*100)\n",
    "        \n",
    "        \n",
    "        #NOUN-LOW-COUNT PROPORTION\n",
    "        noun_count_list.append((noun_freq_count/len(text))*100)\n",
    "        \n",
    "       \n",
    "        #NO-NOUN PROPORTION\n",
    "        no_noun_list.append((no_noun_freq_count/len(text))*100)\n",
    "\n",
    "        \n",
    "        #ADVERB PROPORTION\n",
    "        adverb_count_list.append((adverb_freq_count/len(text))*100)\n",
    "        \n",
    "       \n",
    "    #displays proportion of words (all types) with lower frequency, spanning all speeches  \n",
    "    plt.plot(count_list)\n",
    "    plt.show()\n",
    "       \n",
    "    #ONLY ADJ\n",
    "    plt.plot(adj_count_list)\n",
    "    plt.show()   \n",
    "    \n",
    "    #ONLY VERB\n",
    "    plt.plot(verb_count_list)\n",
    "    plt.show()    \n",
    "        \n",
    "    #ADJ+VERB\n",
    "    a_v = [x + y for x, y in zip(adj_count_list, verb_count_list)]\n",
    "    \n",
    "    plt.plot(a_v)\n",
    "    plt.show()   \n",
    "    \n",
    "    #EVERYTHING APART FROM NOUN\n",
    "    plt.plot(no_noun_list)\n",
    "    plt.show()   \n",
    "           \n",
    "    #ONLY NOUN\n",
    "    plt.plot(noun_count_list)\n",
    "    plt.show()\n",
    "        \n",
    "    #ADVERB\n",
    "    plt.plot(adverb_count_list)\n",
    "    plt.show()\n",
    "  \n",
    "    \n",
    "    \n",
    "    print('fdist') \n",
    "    print(fdist)\n",
    "    print('len')\n",
    "    print(len(fdist))\n",
    "    print('total_len')\n",
    "    print(len(text))\n",
    "\n",
    "\n",
    "\n",
    "def fill_list_of_fileids(list_of_fileids):    \n",
    "    #accesses all text files from the British National Corpus and adds to list of files for frequency analysis \n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\A\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "\n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\B\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\C\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\D\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\E\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "            \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\F\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "            \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\G\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "    \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\H\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "            \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\I\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "            \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\J\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "            \n",
    "    for root, dirs, files in os.walk(os.path.abspath(\"C:\\\\Users\\\\annah\\\\Dropbox\\\\IMPERIAL\\\\INDIVIDUAL PROJECT\\\\jupyter_programs\\\\linguistic_analysis\\\\2554\\\\download\\\\Texts\\\\K\")):\n",
    "        for file in files:\n",
    "            new_dirs = (os.path.join(root, file))\n",
    "            #print (new_dirs)\n",
    "            list_of_fileids.append(new_dirs)\n",
    "            \n",
    "def clean_data(f, newcorpus):\n",
    "    \n",
    "    #cleans and preps data for analysis \n",
    "    \n",
    "    # define punctuation\n",
    "    punctuations = '''!()-–[]{};:'\"“”\\,<>’./?@#$%^&*_~'''\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #get raw text\n",
    "    raw_text = newcorpus.raw(f)\n",
    "        \n",
    "    #get words (between white spaces)\n",
    "    words = raw_text.split()\n",
    "        \n",
    "    #strip punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "        \n",
    "    #strip non-alphabetics\n",
    "    alpha_stripped = [word for word in stripped if word.isalpha()]\n",
    "        \n",
    "    #convert everything to lower case\n",
    "    lower_stripped = [word.lower() for word in alpha_stripped]\n",
    "                \n",
    "    #lemmatise\n",
    "    #list_v = ([lemmatizer.lemmatize(t) for t in lower_stripped])\n",
    "        \n",
    "    #convert list back to string\n",
    "    str1 = ' '.join(lower_stripped)\n",
    "    return str1\n",
    " \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ce48075e8c16>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ce48075e8c16>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    jupyter notebook --generate-config\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter notebook --generate-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
