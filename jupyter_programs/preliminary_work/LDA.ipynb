{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e136bfb79929>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnglish\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwordnet\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "#working version is on pycharm on university computer\n",
    "\n",
    "#on university computer it computes 5 overall topics (with speeches conflated)\n",
    "# -other version computes 5 topics PER speech\n",
    "#results are in the \"summaries\" directory \n",
    "#topics it gives are the areas of text to focus on (perhaps)\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "import os\n",
    "import gensim\n",
    "import pickle\n",
    "import spacy\n",
    "import nltk\n",
    "import codecs\n",
    "\n",
    "spacy.load('en')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    parser = English()\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "\n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def main():\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "    text_data = []\n",
    "\n",
    "    directory = '/homes/ahr18/PycharmProjects/topic_practice/Source/data'\n",
    "\n",
    "    os.chdir(directory)\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            print(filename)\n",
    "            f = open(filename)\n",
    "            #lines = f.read()\n",
    "            with codecs.open(filename, 'r', encoding='utf-8') as fg:\n",
    "                for line in fg:\n",
    "                    #prep the data\n",
    "                    tokens = prepare_text_for_lda(line)\n",
    "                    text_data.append(tokens)\n",
    "\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "    dictionary.save('dictionary.gensim')\n",
    "\n",
    "    #gets 5 topics for the entire set of speeches\n",
    "    #in other version, this code is within the for loop and occurs for each\n",
    "    #individual speech, giving 5 topics per speech (in summaries folder)\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('model5.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=4)\n",
    "\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\annah\\anaconda4\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\annah\\anaconda4\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (7.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (0.0.7)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (0.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (2.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (2.21.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (0.2.2)\n",
      "Requirement already satisfied: jsonschema<3.1.0,>=2.6.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (1.16.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from thinc<7.1.0,>=7.0.2->spacy) (4.31.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.3.9)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (0.14.11)\n",
      "Requirement already satisfied: setuptools in c:\\users\\annah\\anaconda4\\lib\\site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (40.8.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\annah\\anaconda4\\lib\\site-packages (from jsonschema<3.1.0,>=2.6.0->spacy) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
