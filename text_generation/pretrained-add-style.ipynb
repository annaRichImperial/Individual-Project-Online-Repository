{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded domain text: 1905\n",
      "Random example: I never thought it would be possible in my lifetime to join with the Patriarch of Moscow and his congregation in a service in that wonderful cathedral in the heart of the Moscow Kremlin.\n",
      "Splitting to train and validation sets\n",
      "Train: 1333\tValid: 572\n",
      "Text Pre-processing...\n",
      "Num train tokens: 33280\t valid tokens:14232\n",
      "using pretrained\n",
      "loading pre-trained model from data_inf\\pretrained\\fwd_wt103.h5 ...\n",
      "done\n",
      "Tuning the last layer for 1 epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48be7f165c742ee96753182939a6fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                             \n",
      "    0      4.455579   4.284167   0.227377  \n",
      "\n",
      "\n",
      "Fine-tuning for 30 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8406ffd9add84339803d3d6f9d5da80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                             \n",
      "    0      4.117271   3.9271     0.226061  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0221a569a214a26b8fcfa965060f0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=30, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                                                                             \n",
      "    0      4.159873   3.735189   0.262208  \n",
      "    1      3.911696   3.599484   0.272617                                                                             \n",
      "    2      3.643517   3.564383   0.276812                                                                             \n",
      "    3      3.312082   3.626032   0.272572                                                                             \n",
      "    4      2.926667   3.785276   0.264036                                                                             \n",
      "    5      2.552903   3.970139   0.252562                                                                             \n",
      "    6      2.187722   4.15464    0.249636                                                                             \n",
      "    7      1.862202   4.366493   0.244805                                                                             \n",
      "    8      1.672866   4.32201    0.238877                                                                             \n",
      "    9      1.437784   4.621912   0.230765                                                                             \n",
      "    10     1.204221   4.786081   0.23243                                                                              \n",
      "    11     1.037873   4.937669   0.232334                                                                             \n",
      "    12     0.904764   5.028659   0.22729                                                                              \n",
      "    13     0.788702   5.166415   0.231195                                                                             \n",
      "    14     0.71752    5.184471   0.235794                                                                             \n",
      "    15     0.710466   5.195574   0.227742                                                                             \n",
      "    16     0.584399   5.372648   0.234383                                                                             \n",
      "    17     0.530534   5.425513   0.231041                                                                             \n",
      "    18     0.462192   5.581551   0.235179                                                                             \n",
      "    19     0.435686   5.582649   0.232373                                                                             \n",
      "    20     0.391132   5.681951   0.232934                                                                             \n",
      "    21     0.414281   5.605453   0.22976                                                                              \n",
      "    22     0.465461   5.604387   0.227509                                                                             \n",
      "    23     0.373731   5.71129    0.230746                                                                             \n",
      "    24     0.320941   5.766492   0.228963                                                                             \n",
      "    25     0.387645   5.730347   0.228204                                                                             \n",
      "    26     0.325837   5.737529   0.22835                                                                              \n",
      "    27     0.273501   5.778185   0.229759                                                                             \n",
      "    28     0.281736   5.76992    0.228896                                                                             \n",
      "    29     0.26692    5.789995   0.228786                                                                             \n",
      "\n",
      "Saving model as all_speeches_test_lm_30epochs...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from fastai_old.text import *\n",
    "\n",
    "import html\n",
    "\n",
    "import spacy \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "\n",
    "import argparse\n",
    "\n",
    "import bcolz\n",
    "\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "\n",
    "spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "# Define help functions\n",
    "\n",
    "def str2bool(v):\n",
    "\n",
    "    if isinstance(v, bool):\n",
    "\n",
    "        return v\n",
    "\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "\n",
    "        return True\n",
    "\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "        \n",
    "\n",
    "def get_texts(fname, minwords = 100):\n",
    "\n",
    "    texts = []\n",
    "\n",
    "    with fname.open('r', encoding='utf-8') as f:\n",
    "\n",
    "        curr = ['', 0]\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            if line == '\\n': # giannis\n",
    "\n",
    "                continue\n",
    "\n",
    "            l = len(line.split(' '))\n",
    "\n",
    "            if curr[1] + l > minwords:\n",
    "\n",
    "                texts.append(curr[0])\n",
    "\n",
    "                curr = [line, l]\n",
    "\n",
    "            else:\n",
    "\n",
    "                curr[0] += '\\n' + line\n",
    "\n",
    "                curr[1] += l\n",
    "\n",
    "    if curr[0] != '':\n",
    "\n",
    "        texts.append(curr[0])\n",
    "\n",
    "    return np.array(texts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_sentences(fname):\n",
    "\n",
    "    # Giannis: splitting text into sentences instead of paragraphs. \n",
    "\n",
    "    texts = []\n",
    "\n",
    "    with fname.open('r', encoding='utf-8') as f:\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            if line == '\\n': continue\n",
    "\n",
    "            line = re.sub(r\"\\[[0-9]+\\]\", \"\", line) # replace \"[<int>]\" by \"\" (common in dfw_lobster)\n",
    "\n",
    "            sents = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', line.strip())\n",
    "\n",
    "            texts.extend(sents)\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fixup(x):\n",
    "\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ').replace('.',' .').replace('?',' ?').replace('!',' !').replace('’',\" '\")\n",
    "\n",
    "    re1 = re.compile(r'  +')\n",
    "\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(all_texts):\n",
    "\n",
    "    col_names = ['text']\n",
    "\n",
    "    df = pd.DataFrame({'text':all_texts}, columns=col_names)\n",
    "\n",
    "    texts = df['text'].astype(str)\n",
    "\n",
    "    texts = list(texts.apply(fixup).values)\n",
    "    \n",
    "    s = partition_by_cores(texts)\n",
    "\n",
    "    tok = Tokenizer().proc_all_mp(s)\n",
    "\n",
    "    return tok\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_general_vocab(vocab_path):\n",
    "\n",
    "    itos = pickle.load((vocab_path).open('rb'))\n",
    "\n",
    "    stoi = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "    return itos, stoi\n",
    "\n",
    "\n",
    "\n",
    "def get_domain_vocab(tok, min_freq, max_freq):\n",
    "\n",
    "    freq = Counter(p for o in tok for p in o)\n",
    "\n",
    "    itos = [o for o,c in freq.most_common(max_freq) if c>min_freq]\n",
    "\n",
    "    itos.insert(0, '_pad_')\n",
    "\n",
    "    itos.insert(0, '_unk_')\n",
    "\n",
    "    stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "    return itos, stoi\n",
    "\n",
    "\n",
    "\n",
    "def print_stats(itos, itos2):\n",
    "\n",
    "    print(\"itos: {}\".format(len(itos)))\n",
    "\n",
    "    print(\"itos2: {}\".format(len(itos2)))\n",
    "\n",
    "    print(\"itos2 - itos: {}\".format(len(set(itos2)-set(itos))))\n",
    "\n",
    "    print(\"itos - itos2: {}\".format(len(set(itos)-set(itos2))))\n",
    "\n",
    "    print(\"itos & itos2: {}\".format(len(set(itos).union(set(itos2)))))  \n",
    "\n",
    "    #unseen = set(itos) - set(itos2)\n",
    "\n",
    "    #print(\"Domain words not seen previously ({} in total): {}\".format(len(unseen), unseen))\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def merge_vocab(itos_domain, itos_general, sample_general):\n",
    "\n",
    "    print(\"Merging vocabularies\")\n",
    "\n",
    "    if sample_general != -1:\n",
    "\n",
    "        itos_general = random.sample(itos_general, sample_general)\n",
    "\n",
    "    itos = list(set(itos_domain).union(itos_general))\n",
    "\n",
    "    stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "    print(\"New vocab size = {}\".format(len(itos)))\n",
    "\n",
    "    return itos, stoi\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_pretrained_weights(itos, stoi2, pre_lm_path, vs, em_sz):\n",
    "\n",
    "    print('loading pre-trained model from {} ...'.format(pre_lm_path))\n",
    "\n",
    "    wgts = torch.load(pre_lm_path, map_location=lambda storage, loc: storage)\n",
    "\n",
    "\n",
    "\n",
    "    enc_wgts = to_np(wgts['0.encoder.weight'])\n",
    "\n",
    "    row_m = enc_wgts.mean(0)\n",
    "\n",
    "\n",
    "\n",
    "    new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "\n",
    "    for i,w in enumerate(itos):                     # for word in imbd vocab\n",
    "\n",
    "        r = stoi2[w]                                # get the int in the pretrained vocab\n",
    "\n",
    "        new_w[i] = enc_wgts[r] if r>=0 else row_m   # add weight if in vocab, else add mean weight\n",
    "\n",
    "\n",
    "\n",
    "    wgts['0.encoder.weight'] = T(new_w)\n",
    "\n",
    "    wgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\n",
    "\n",
    "    wgts['1.decoder.weight'] = T(np.copy(new_w))\n",
    "\n",
    "    print('done')\n",
    "\n",
    "    return wgts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_lm(lm_path, trn_data, val_data, vs, em_sz, bs):\n",
    "\n",
    "    nh,nl = 1150,3\n",
    "\n",
    "    bptt=70\n",
    "\n",
    "    opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "\n",
    "\n",
    "\n",
    "    trn_dl = LanguageModelLoader(np.concatenate(trn_data), bs, bptt)\n",
    "\n",
    "    val_dl = LanguageModelLoader(np.concatenate(val_data), bs, bptt)\n",
    "\n",
    "    md = LanguageModelData(lm_path, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "\n",
    "\n",
    "\n",
    "    drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "\n",
    "\n",
    "\n",
    "    learner = md.get_model(opt_fn, em_sz, nh, nl, \n",
    "\n",
    "        dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[3], dropouth=drops[4])\n",
    "\n",
    "    return learner\n",
    "\n",
    "\n",
    "\n",
    "def main(num_epochs, valid_size, use_general, sample_general, use_pretrained, batch_size):\n",
    "\n",
    "    # load domain text\n",
    "    \n",
    "    all_texts = []\n",
    "    '''\n",
    "    \n",
    "    from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "    \n",
    "    newcorpus = PlaintextCorpusReader('data_style/data', '.*')\n",
    "    filenames = newcorpus.fileids()\n",
    "    \n",
    "    for f in filenames:\n",
    "        print(f)\n",
    "        data_file = NAME + f \n",
    "        all_texts += get_sentences(DATA_PATH/data_file)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    data_file = NAME + '.txt'\n",
    "    all_texts = get_sentences(DATA_PATH/data_file)\n",
    "    \n",
    "    print(\"Loaded domain text: {}\".format(len(all_texts)))\n",
    "\n",
    "    print(\"Random example: {}\".format(random.choice(all_texts)))\n",
    "\n",
    "\n",
    "\n",
    "    #     giannis FIXME: THIS JUST DUPLICATES OUR DATA. USE ONLY FOR DEBUGGING REASONS\n",
    "\n",
    "    #x = list(all_texts)\n",
    "\n",
    "    #x.extend(x)\n",
    "\n",
    "    #x.extend(x)\n",
    "\n",
    "    #x.extend(x)\n",
    "\n",
    "    #all_texts = np.array(x)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Splitting to train and validation sets\")\n",
    "\n",
    "    trn_texts,val_texts = sklearn.model_selection.train_test_split(\n",
    "\n",
    "        all_texts, test_size=valid_size)\n",
    "\n",
    "    print(\"Train: {}\\tValid: {}\".format(len(trn_texts), len(val_texts)))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Text Pre-processing...\")\n",
    "\n",
    "    tok_trn = preprocess(trn_texts)\n",
    "\n",
    "    tok_val = preprocess(val_texts)\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Num train tokens: {}\\t valid tokens:{}\".format(len(np.concatenate(tok_trn)), len(np.concatenate(tok_val))))\n",
    "\n",
    "\n",
    "\n",
    "    # Loading general & domain vocabularies\n",
    "\n",
    "\n",
    "    itos_filename = 'itos_' + NAME + ID + '.pkl'\n",
    "\n",
    "    itos_file = LM_PATH/itos_filename\n",
    "\n",
    "    general_vocab_path = PRE_PATH/'itos_wt103.pkl'\n",
    "\n",
    "\n",
    "\n",
    "    itos, stoi = get_domain_vocab(tok_trn, min_freq=1, max_freq=60000)\n",
    "\n",
    "    itos2, stoi2 = get_general_vocab(general_vocab_path)\n",
    "\n",
    "    if use_general:\n",
    "\n",
    "        itos, stoi = merge_vocab(itos, itos2, sample_general=sample_general)\n",
    "\n",
    "    with open(itos_file, 'wb') as f:\n",
    "\n",
    "        pickle.dump(itos, f)\n",
    "\n",
    "\n",
    "\n",
    "    # Converting text to indices\n",
    "\n",
    "    trn_data = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "\n",
    "    val_data = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "\n",
    "\n",
    "    # Loading pre-trained model\n",
    "\n",
    "    PRE_LM_PATH = PRE_PATH/'fwd_wt103.h5'\n",
    "\n",
    "    em_sz = 400\n",
    "\n",
    "    vs = len(itos)\n",
    "\n",
    "    learner = build_lm(LM_PATH, trn_data, val_data, vs, em_sz, batch_size)\n",
    "\n",
    "    if use_pretrained:\n",
    "\n",
    "        print('using pretrained')\n",
    "\n",
    "        wgts = get_pretrained_weights(itos, stoi2, PRE_LM_PATH, vs, em_sz)\n",
    "\n",
    "        learner.model.load_state_dict(wgts)      \n",
    "\n",
    "\n",
    "\n",
    "    # Fine-tuning \n",
    "\n",
    "    learner.metrics = [accuracy]\n",
    "\n",
    "    learner.freeze_to(-1)\n",
    "\n",
    "    wd = 1e-7\n",
    "\n",
    "    lr = 1e-3\n",
    "\n",
    "    lrs = lr\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Tuning the last layer for 1 epoch\")\n",
    "\n",
    "    learner.fit(lrs/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)\n",
    "\n",
    "    learner.save(NAME + ID + '_lm_last_ft')\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n\\nFine-tuning for {} epochs\".format(num_epochs))\n",
    "\n",
    "    learner.unfreeze()\n",
    "\n",
    "    learner.lr_find(start_lr=lrs/10, end_lr=lrs*10, linear=True)\n",
    "\n",
    "    learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=num_epochs)\n",
    "\n",
    "    learner.sched.plot_loss()\n",
    "\n",
    "    lm_name = NAME + ID + '_lm_' + str(num_epochs) + 'epochs'\n",
    "\n",
    "    print(\"\\nSaving model as {}...\".format(lm_name))\n",
    "\n",
    "    learner.save(lm_name)\n",
    "\n",
    "\n",
    "\n",
    "#     num_epochs3 = 5\n",
    "\n",
    "#     learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=num_epochs3)\n",
    "\n",
    "#     learner.sched.plot_loss()\n",
    "\n",
    "#     #lm_name = NAME + ID + '_lm_' + str(num_epochs + num_epochs2 + num_epochs3) + 'epochs'\n",
    "\n",
    "#     lm_name = NAME + ID + '_lm_' + str(num_epochs + num_epochs3) + 'epochs'\n",
    "\n",
    "#     learner.save(lm_name)\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"\\nSaving model as {}...\".format(SAVE_NAME))\n",
    "\n",
    "#     learner.save(SAVE_NAME)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Define paths & names\n",
    "\n",
    "    #parser = argparse.ArgumentParser(description='STYLE GENERATION EXPERIMENTS')\n",
    "\n",
    "    #parser.add_argument('--NAME', nargs='?', type=str, help='Book name', required=False, default='dfw_lobster')\n",
    "\n",
    "    #parser.add_argument('--num_epochs', nargs='?', type=int, help='Number of epochs', required=False, default=30)\n",
    "\n",
    "    #parser.add_argument('--valid_size', nargs='?', type=float, help='Validation size (0-1)', required=False, default=0.3)\n",
    "\n",
    "    #parser.add_argument('--use_general', nargs='?', help='Use general vocabulary?', const=True, required=False, default=False)\n",
    "\n",
    "    #parser.add_argument('--sample_general', nargs='?', type=int, help='Number of samples to keep from general vocabulary', required=False, default=10000)\n",
    "\n",
    "    #parser.add_argument('--use_pretrained', nargs='?', help='Use pretrained model?', const=True, required=False, default=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #args.use_general = str2bool(args.use_general)\n",
    "\n",
    "    #args.use_pretrained = str2bool(args.use_pretrained)\n",
    "\n",
    "    #print('Arguments: {}'.format(args))\n",
    "\n",
    "    \n",
    "    #DATA_PATH_ALT=Path('data_style/data')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #data_file_alt = DATA_PATH_ALT + '.txt'\n",
    "    #all_texts = get_sentences(DATA_PATH/data_file)\n",
    "    '''\n",
    "    with open(\"data_style/result.txt\", \"w\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(DATA_PATH_ALT/f, \"r\") as infile:\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "   # with open('data_style/output_file', 'w') as outfile:\n",
    "    #    for fname in filenames:\n",
    "     #       with open(DATA_PATH_ALT/fname) as infile:\n",
    "      #          for line in infile:\n",
    "       #             outfile.write(line)\n",
    "                \n",
    "\n",
    "    NAME = 'all_speeches'\n",
    "\n",
    "    # giannis: for dfw_lobster we need to manually set a batch size <= number of validation data\n",
    "\n",
    "    if NAME == 'dfw_lobster': \n",
    "\n",
    "        batch_size = 10\n",
    "\n",
    "    elif NAME == '1952':\n",
    "        \n",
    "        batch_size = 10\n",
    "    \n",
    "    else:\n",
    "\n",
    "        batch_size = 10\n",
    "\n",
    "\n",
    "\n",
    "    num_epochs = 30\n",
    "    valid_size = 0.3\n",
    "    use_general = False\n",
    "    sample_general = 10000\n",
    "    use_pretrained = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    DATA_PATH=Path('data_style/')\n",
    "\n",
    "    DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "    #LM_PATH = Path('data_inf/custom_lm/{}/'.format(datetime.now().strftime('%b%d_%H-%M-%S')))\n",
    "\n",
    "\n",
    "    LM_PATH = Path('data_inf/custom_lm/')\n",
    "\n",
    "    LM_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "    PRE_PATH=Path('data_inf/pretrained/')\n",
    "\n",
    "    PRE_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "    ID = '_test'\n",
    "\n",
    "    SAVE_NAME = '{}_lm_{}'.format(NAME, num_epochs)\n",
    "\n",
    "    main(num_epochs, valid_size, use_general, sample_general, use_pretrained, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
