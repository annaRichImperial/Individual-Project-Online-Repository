{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters:  2150\n",
      "total vocab:  26\n",
      "total patterns  2050\n",
      "Epoch 1/20\n",
      "2050/2050 [==============================] - 89s 43ms/step - loss: 3.1144\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.11444, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/20\n",
      "2050/2050 [==============================] - 65s 32ms/step - loss: 2.9980\n",
      "\n",
      "Epoch 00002: loss improved from 3.11444 to 2.99798, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/20\n",
      "2050/2050 [==============================] - 71s 35ms/step - loss: 2.9753\n",
      "\n",
      "Epoch 00003: loss improved from 2.99798 to 2.97527, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/20\n",
      "2050/2050 [==============================] - 69s 34ms/step - loss: 2.9559\n",
      "\n",
      "Epoch 00004: loss improved from 2.97527 to 2.95593, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/20\n",
      "2050/2050 [==============================] - 79s 39ms/step - loss: 2.9600\n",
      "\n",
      "Epoch 00005: loss did not improve from 2.95593\n",
      "Epoch 6/20\n",
      "2050/2050 [==============================] - 79s 39ms/step - loss: 2.9487\n",
      "\n",
      "Epoch 00006: loss improved from 2.95593 to 2.94874, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/20\n",
      "2050/2050 [==============================] - 88s 43ms/step - loss: 2.9364\n",
      "\n",
      "Epoch 00007: loss improved from 2.94874 to 2.93636, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/20\n",
      "2050/2050 [==============================] - 86s 42ms/step - loss: 2.9388\n",
      "\n",
      "Epoch 00008: loss did not improve from 2.93636\n",
      "Epoch 9/20\n",
      "2050/2050 [==============================] - 91s 45ms/step - loss: 2.9396\n",
      "\n",
      "Epoch 00009: loss did not improve from 2.93636\n",
      "Epoch 10/20\n",
      "2050/2050 [==============================] - 119s 58ms/step - loss: 2.9444\n",
      "\n",
      "Epoch 00010: loss did not improve from 2.93636\n",
      "Epoch 11/20\n",
      "2050/2050 [==============================] - 160s 78ms/step - loss: 2.9234\n",
      "\n",
      "Epoch 00011: loss improved from 2.93636 to 2.92338, saving model to model_weights_saved.hdf5\n",
      "Epoch 12/20\n",
      "2050/2050 [==============================] - 172s 84ms/step - loss: 2.9360\n",
      "\n",
      "Epoch 00012: loss did not improve from 2.92338\n",
      "Epoch 13/20\n",
      "2050/2050 [==============================] - 136s 66ms/step - loss: 2.9334\n",
      "\n",
      "Epoch 00013: loss did not improve from 2.92338\n",
      "Epoch 14/20\n",
      "2050/2050 [==============================] - 134s 65ms/step - loss: 2.9343\n",
      "\n",
      "Epoch 00014: loss did not improve from 2.92338\n",
      "Epoch 15/20\n",
      "2050/2050 [==============================] - 142s 69ms/step - loss: 2.9372\n",
      "\n",
      "Epoch 00015: loss did not improve from 2.92338\n",
      "Epoch 16/20\n",
      "2050/2050 [==============================] - 96s 47ms/step - loss: 2.9312\n",
      "\n",
      "Epoch 00016: loss did not improve from 2.92338\n",
      "Epoch 17/20\n",
      "2050/2050 [==============================] - 95s 47ms/step - loss: 2.9277\n",
      "\n",
      "Epoch 00017: loss did not improve from 2.92338\n",
      "Epoch 18/20\n",
      "2050/2050 [==============================] - 84s 41ms/step - loss: 2.9261\n",
      "\n",
      "Epoch 00018: loss did not improve from 2.92338\n",
      "Epoch 19/20\n",
      "2050/2050 [==============================] - 81s 39ms/step - loss: 2.9343\n",
      "\n",
      "Epoch 00019: loss did not improve from 2.92338\n",
      "Epoch 20/20\n",
      "2050/2050 [==============================] - 79s 38ms/step - loss: 2.9395\n",
      "\n",
      "Epoch 00020: loss did not improve from 2.92338\n",
      "random seed: \n",
      "\" alty affection immense support encouragement want take christmas day first opportunity thank heart m \"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
     ]
    }
   ],
   "source": [
    "#first practice of text generation - to familiarise myself\n",
    "#character prediction \n",
    "\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #MAKE IT ALL FILES\n",
    "    file = open(\"data/1952.txt\").read()\n",
    "    #file = open(\"\").read()\n",
    "    processed = tokenize_words(file)\n",
    "    set_p = set(processed)\n",
    "    list_p = list(set_p)\n",
    "    chars = sorted(list_p)\n",
    "    char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "    \n",
    "    input_len = len(processed)\n",
    "    vocab_len = len(chars)\n",
    "    print('total characters: ', input_len)\n",
    "    print('total vocab: ', vocab_len)\n",
    "    \n",
    "    seq_length = 100\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(0, input_len - seq_length, 1):\n",
    "        in_seq = processed[i:i + seq_length]\n",
    "        out_seq = processed[i + seq_length]\n",
    "        x_data.append([char_to_num[char] for char in in_seq])\n",
    "        y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "    n_patterns = len(x_data)\n",
    "    print('total patterns ', n_patterns)\n",
    "    \n",
    "    X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "    X = X/float(vocab_len)\n",
    "    y = np_utils.to_categorical(y_data)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    filepath = \"model_weights_saved.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    desired_callbacks = [checkpoint]\n",
    "    \n",
    "    model.fit(X, y, epochs=20, batch_size=256, callbacks=desired_callbacks)\n",
    "    \n",
    "    filename = \"model_weights_saved.hdf5\"\n",
    "    model.load_weights(filename)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    \n",
    "    start = numpy.random.randint(0, len(x_data) - 1)\n",
    "    pattern = x_data[start]\n",
    "    print(\"random seed: \")\n",
    "    print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")\n",
    "    \n",
    "    for i in range(1000):\n",
    "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(vocab_len)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = num_to_char[index]\n",
    "        seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "        sys.stdout.write(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "   \n",
    "    \n",
    "    \n",
    "    #increase epochs for better reuslts \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
