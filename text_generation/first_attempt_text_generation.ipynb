{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0719 09:28:04.939786   744 deprecation_wrapper.py:119] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0719 09:28:05.010596   744 deprecation_wrapper.py:119] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0719 09:28:05.030542   744 deprecation_wrapper.py:119] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total characters:  2150\n",
      "total vocab:  26\n",
      "total patterns  2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 09:28:05.822425   744 deprecation_wrapper.py:119] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0719 09:28:05.836389   744 deprecation.py:506] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0719 09:28:06.673152   744 deprecation_wrapper.py:119] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0719 09:28:06.720025   744 deprecation_wrapper.py:119] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0719 09:28:06.929468   744 deprecation.py:323] From C:\\Users\\annah\\Anaconda4\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2050/2050 [==============================] - 63s 31ms/step - loss: 3.1004\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.10040, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "2050/2050 [==============================] - 58s 28ms/step - loss: 2.9712\n",
      "\n",
      "Epoch 00002: loss improved from 3.10040 to 2.97121, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "2050/2050 [==============================] - 58s 28ms/step - loss: 2.9565\n",
      "\n",
      "Epoch 00003: loss improved from 2.97121 to 2.95645, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "2050/2050 [==============================] - 66s 32ms/step - loss: 2.9560\n",
      "\n",
      "Epoch 00004: loss improved from 2.95645 to 2.95605, saving model to model_weights_saved.hdf5\n",
      "random seed: \n",
      "\" rnal message christmas desire us coronation next june shall dedicate anew service shall presence gre \"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
     ]
    }
   ],
   "source": [
    "#first practice of text generation - to familiarise myself\n",
    "#character prediction \n",
    "\n",
    "import numpy\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #MAKE IT ALL FILES\n",
    "    file = open(\"data/1952.txt\").read()\n",
    "    #file = open(\"\").read()\n",
    "    processed = tokenize_words(file)\n",
    "    set_p = set(processed)\n",
    "    list_p = list(set_p)\n",
    "    chars = sorted(list_p)\n",
    "    char_to_num = dict((c, i) for i, c in enumerate(chars))\n",
    "    \n",
    "    input_len = len(processed)\n",
    "    vocab_len = len(chars)\n",
    "    print('total characters: ', input_len)\n",
    "    print('total vocab: ', vocab_len)\n",
    "    \n",
    "    seq_length = 100\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    \n",
    "    for i in range(0, input_len - seq_length, 1):\n",
    "        in_seq = processed[i:i + seq_length]\n",
    "        out_seq = processed[i + seq_length]\n",
    "        x_data.append([char_to_num[char] for char in in_seq])\n",
    "        y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "    n_patterns = len(x_data)\n",
    "    print('total patterns ', n_patterns)\n",
    "    \n",
    "    x = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "    x = x/float(vocab_len)\n",
    "    y = np_utils.to_categorical(y_data)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    filepath = \"model_weights_saved.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    desired_callbacks = [checkpoint]\n",
    "    \n",
    "    model.fit(x, y, epochs=4, batch_size=256, callbacks=desired_callbacks)\n",
    "    \n",
    "    filename = \"model_weights_saved.hdf5\"\n",
    "    model.load_weights(filename)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    num_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    \n",
    "    start = numpy.random.randint(0, len(x_data) - 1)\n",
    "    pattern = x_data[start]\n",
    "    print(\"random seed: \")\n",
    "    print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")\n",
    "    \n",
    "    \n",
    "    for i in range(1000):\n",
    "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(vocab_len)\n",
    "        prediction = model.predict(x, verbose=0)\n",
    "        index = numpy.argmax(prediction)\n",
    "        result = num_to_char[index]\n",
    "        seq_in = [num_to_char[value] for value in pattern]\n",
    "        \n",
    "        sys.stdout.write(result)\n",
    "        \n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "    \n",
    "    #increase epochs for better reuslts \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
